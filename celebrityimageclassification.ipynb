{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "64fc34a4",
   "metadata": {},
   "source": [
    "# This project follows correct step-by-step methods. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16076136-35de-4b4b-9852-7dcde9c2e0f9",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install mtcnn\n",
    "!pip install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22d858ed",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "from mtcnn import MTCNN # Multi Task Cascaded Convolution Networks\n",
    "\n",
    "# Paths\n",
    "raw_path = \"/kaggle/input/celebrity-image-classification-dataset2/data\"\n",
    "cropped_path = \"/kaggle/working/celebrity-image-classification/data_cropped\"\n",
    "\n",
    "detector = MTCNN() # for detecting only the face in image by cascading\n",
    "\n",
    "for celeb in os.listdir(raw_path): #  os.listdir(raw_path) will list all the files and folders inside raw_path\n",
    "    celeb_raw_folder = os.path.join(raw_path, celeb) # this will create a proper path for each celeb folder ('data/celeb_folder')\n",
    "    celeb_cropped_folder = os.path.join(cropped_path, celeb) # specifying a path (location) to save the cropped faces of each celeb\n",
    "    os.makedirs(celeb_cropped_folder, exist_ok = True) # makedirs(celeb_cropped_folder) creates that folder for the path. exist_ok = True will not throw error if folder already exists \n",
    "\n",
    "\n",
    "    # Loop through each image\n",
    "    for img_name in os.listdir(celeb_raw_folder): # lists all the files (images) in that celebrity's image folder.\n",
    "        img_path = os.path.join(celeb_raw_folder, img_name) # selects the full path of each image of that celeb folder\n",
    "        img = cv2.imread(img_path) # loads the image into memory as a NumPy array. If the file valid image(jpg, png), you'll get a matrix of pixel values (shape: height, width, for 3 RGB colors)\n",
    "        #if the image is corrupted or in wrong format, then it will return None.\n",
    "        \n",
    "        if img is None: # sometimes datasets have broken text files, images or hidden system files. This check avoid errors by skipping invalid files and moves on.\n",
    "            continue \n",
    "        \n",
    "        # here, the color of the image is being changed. Color image has 3 channels: Blue, Green, Red and shape: height, width, 3. A grayscale image has 1 channel: intensity, and shape: height, width which means the grey color is visible by changing the intensity of the image instead of colors\n",
    "\n",
    "        faces = detector.detect_faces(img) # Given image to detect only face image stays in the same color (BGR)\n",
    "        \n",
    "        if faces: # If at leaset one face detected then run the below condition. Also, the 'face' here, returns a dictionary which stores the bounding box (the image rectangle), confidence(how correct it is classified as a face), and keypoints(the landmarks of each part: e.g., nose, eyes)\n",
    "            \n",
    "            face = max(faces, key = lambda f: f['box'][2]*f['box'][3]) # Selects the face(if multiple face per image) with the largest area(likely the main celeb) using h*w. This will avoid the model to select background images which can distrupt the model's learning.\n",
    "            x, y, w, h = face['box']  # Here, each tuple (x, y, w, h) means - x, y is the top left corner of the face rectangle(box) and w, h is the width and height of the rectangle.\n",
    "            \n",
    "            x, y = max(0, x), max(0, y) # Useful if image is near the edge and the coordinates are detected -vly. So, max(0, x) stores the larger value only for x. max(0, x) compares both 0 and x and - if x > 0, x is stored, else 0.\n",
    "            \n",
    "            face_img = img[y:y+h, x:x+w] # slices(crops) the image to only extract the face region. face_imag is now a NumPy array containing only that face. (check the Brazil-covered notebook for further details).\n",
    "            \n",
    "            # “Use the original image name plus a number to save each cropped face uniquely in the right folder.”\n",
    "            save_path = os.path.join(celeb_cropped_folder, f\"{img_name.split('.')[0]}.jpg\") # img_name.split('.')[0] takes the file name without extension.\n",
    "\n",
    "\n",
    "            cv2.imwrite(save_path, face_img) # writes the image into the image path\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bcec997",
   "metadata": {},
   "source": [
    "## Preprocessing for CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "be30dbc7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-01T07:54:06.497115Z",
     "iopub.status.busy": "2025-10-01T07:54:06.496874Z",
     "iopub.status.idle": "2025-10-01T07:54:10.324395Z",
     "shell.execute_reply": "2025-10-01T07:54:10.323835Z",
     "shell.execute_reply.started": "2025-10-01T07:54:06.497098Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-01 07:54:06.787274: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1759305246.809843    3432 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1759305246.816867    3432 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 545 images belonging to 8 classes.\n",
      "Found 130 images belonging to 8 classes.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# these datagen's parameters(rescale, rotation_range, etc) are used during the training and the size of the training data will increase because of them. This is given to training because the model has to be trained on the basis of different image patterns so to avoid overfitting.\n",
    "datagen = ImageDataGenerator(\n",
    "    rescale = 1./255, # Since we know that a normal RGB image has pixel values in the range of 0 to 255 for each color channel (R, G, B). rescale = 1./255 divides each pixel value of image by 255 for normalization. E.g., [255,0,0] --> [1.0,0.0,0.0]\n",
    "    rotation_range = 20, # randomly rotates image by 20%\n",
    "    width_shift_range = 0.1, # shifts the image(cropped one) 10% horizontally. \n",
    "    height_shift_range = 0.1, # shifts the image 10% vertically\n",
    "    horizontal_flip = True, # generates a mirrored image. Because the image of that celeb should also be learned as a mirrored image\n",
    "    validation_split = 0.2 # splits dataset into 80% training, and 20% validation. Validation data is not augmented(i.e., these all parameters are not applied; except rescale)\n",
    ")\n",
    "\n",
    "train_gen = datagen.flow_from_directory(\n",
    "    '/kaggle/input/cropped-dataset-of-celebrity-image/data_cropped',              # Path to the dataset folder\n",
    "    target_size = (224, 224),    # Resizes the image(cropped one) into equal size. This size of the image is most commmon for this kind of problems.\n",
    "    batch_size = 32,             # Number of images per batch\n",
    "    class_mode = 'sparse',  # For giving labels to each folder image for multi-classification (one-hot labels). E.g., Akshay - label 1\n",
    "    subset = 'training',          # Mentioned 'training' to use the training split (not the validation)\n",
    "    shuffle = True            # Important for generalization\n",
    ")\n",
    "\n",
    "validation_gen = datagen.flow_from_directory(\n",
    "    '/kaggle/input/cropped-dataset-of-celebrity-image/data_cropped',           # we need to specify it so that train and validation images are not overlapped\n",
    "    target_size = (224, 224),\n",
    "    batch_size = 32, # Number of images per batch\n",
    "    class_mode = 'sparse',   # So that the model specifies the validation is correct/incorrect based on the same labels as of training\n",
    "    subset = 'validation',\n",
    "    shuffle = False        # not required here\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1613e002-ec0c-43cf-a982-1cb1765db7d0",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_gen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a651dd2",
   "metadata": {},
   "source": [
    "### Defining the CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2882bcd",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "\n",
    "num_classes = len(train_gen.class_indices)\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(32, (3,3), activation='relu', input_shape = (224,224,3))) # 32 is 32 no. of filters that will be used for the initial layers. Each filter will be capturing different parts of the image and will give a feature map. Note, the early layers will use all the 32 filters simultaneously for a particular image at a single iteration.     # 3,3 is the filter size. activation function relu. \n",
    "model.add(MaxPooling2D(2,2)) # pooling layer of shape 2,2\n",
    "\n",
    "model.add(Conv2D(64, (3,3), activation = 'relu')) # Now, using 64 filters for the remaining layers to extract more complex things like(shape of nose, eyes, arms) in middle layers.\n",
    "model.add(MaxPooling2D(2,2))\n",
    "\n",
    "\n",
    "model.add(Conv2D(128, (3,3), activation = 'relu'))\n",
    "model.add(MaxPooling2D(2,2))\n",
    "\n",
    "Flatten() # flattens the pooled data\n",
    "\n",
    "\n",
    "model.add(Dense(128, activation = 'relu')) # A dense layer which will form a neural network by taking the flattened data to form a neral network of 128 neurons. Conv2D has filters whereas, Dense has neurons\n",
    "model.add(Dropout(0.5)) # Randomly sets 50% of the neurons to 0 during training to prevent overfitting.\n",
    "\n",
    "model.add(Dense(num_classes, activation = 'softmax')) # Output layer. Here we used no. of neurons = num_classes because, the final prediction should based on the final classes. Predicts proba for each class. softmax predicts the output class. softmax(make sure that the sum of all probas is 1)\n",
    "\n",
    "\n",
    "\n",
    "model.compile(optimizer = 'adam', loss = 'sparse_categorical_crossentropy', metrics = ['accuracy']) # accuracy tells keras to give the accuracy for validation and training at each epoch.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db4700ba",
   "metadata": {},
   "source": [
    "### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce354712-c556-4b07-bdc6-0f2d7cf6dae4",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "\n",
    "# Compute class weights\n",
    "class_weights_array = compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=np.unique(train_gen.classes),# giving this to capture distinct classes in the data.\n",
    "    y=train_gen.classes # Now, '.classes' will compute the class weights for the distinct classes instead of computing weights for all the records of train_gen.\n",
    ")\n",
    "\n",
    "\n",
    "class_weights_dict = dict(enumerate(class_weights_array))\n",
    "\n",
    "\n",
    "history = model.fit(\n",
    "    train_gen,\n",
    "    validation_data = validation_gen,\n",
    "    epochs = 55,\n",
    "    class_weight=class_weights_dict\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5683f066-0e64-4429-86b6-a219bcc42b32",
   "metadata": {},
   "source": [
    "<h1>Hyperparameter Tuning using Bayesian Optimization</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5712dcb1-1275-4d98-b7e0-0de126f866ed",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# To decide how many trials are needed follow these steps:\n",
    "\n",
    "#  Since the rule of thumb to decide the no. of trials which are required to find best model implies --> no. of trials = no. of parameters which are being tunned x 10.\n",
    "# Now, the question comes if the range of the min-max value of the parameter is medium-large, than how many trials are required:\n",
    "                # ✅ Small ranges (e.g. dropout 0.2–0.5, filters 32–128) → stick to n × 10.\n",
    "                # ✅ Medium ranges (filters 32–512, LR 1e-5–1e-2) → go n × 12\n",
    "                # ✅ Very wide ranges (LR 1e-7–1, dense 32–2048) → plan for 100+ trials or reduce the ranges.\n",
    "\n",
    "# If you don't want to try the above steps then we should 1st) reduce the image size(target_size), 2nd) reduce the no. of batch size so that the model can become faster which will prevent less meemory overload and less GPU load because, when we take more batch size in CNN (not ANN) then, the further calculations of filters kernels and padding becomes much more time consuming. So, taking less batch_size makes the model more faster and accurate and prevent this problem.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "216b0736-009e-487f-ac6e-c2ec969cbf7f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-01T07:54:16.705054Z",
     "iopub.status.busy": "2025-10-01T07:54:16.704497Z",
     "iopub.status.idle": "2025-10-01T07:54:16.741631Z",
     "shell.execute_reply": "2025-10-01T07:54:16.740963Z",
     "shell.execute_reply.started": "2025-10-01T07:54:16.705031Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 545 images belonging to 8 classes.\n",
      "Found 130 images belonging to 8 classes.\n"
     ]
    }
   ],
   "source": [
    "train_gen_small = datagen.flow_from_directory(\n",
    "    '/kaggle/input/cropped-dataset-of-celebrity-image/data_cropped',               \n",
    "    target_size = (128, 128),  # smaller target and batch size for fast tuning   \n",
    "    batch_size = 8,              \n",
    "    class_mode = 'sparse',   \n",
    "    subset = 'training',\n",
    "    shuffle = True\n",
    "    \n",
    ")\n",
    "\n",
    "validation_gen_small = datagen.flow_from_directory(\n",
    "    '/kaggle/input/cropped-dataset-of-celebrity-image/data_cropped',            \n",
    "    target_size = (128, 128),\n",
    "    batch_size = 8, \n",
    "    class_mode = 'sparse',    \n",
    "    subset = 'validation',\n",
    "    shuffle = False\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43a79cb6-2af6-4669-8599-07143cee6a77",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "pip install keras-tuner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "053161c1-1dfd-4242-b7c9-4968fbef8b70",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-01T07:54:20.151296Z",
     "iopub.status.busy": "2025-10-01T07:54:20.150542Z",
     "iopub.status.idle": "2025-10-01T07:54:22.427892Z",
     "shell.execute_reply": "2025-10-01T07:54:22.426761Z",
     "shell.execute_reply.started": "2025-10-01T07:54:20.151270Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import keras_tuner as kt\n",
    "import keras\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, GlobalAveragePooling2D\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "num_classes = len(train_gen_small.class_indices)\n",
    "\n",
    "\n",
    "def model_builder(hp):\n",
    "    model2 = Sequential()\n",
    "    \n",
    "    model2.add(Conv2D(\n",
    "        filters = 64,\n",
    "        kernel_size = (3,3), # tuning filter size where the possible sizes to tune are 3*3 or 5*5\n",
    "        activation = \"relu\",\n",
    "        padding = \"same\", # This adds 0 to the extra size to make the ouput of the equal size of the input size\n",
    "        input_shape = (224, 224, 3) # Using the same size of the resized(cropped) size of the images\n",
    "\n",
    "    ))\n",
    "    model2.add(MaxPooling2D(pool_size = 2)) # 2 x 2 pooling shape of the max pooling\n",
    "\n",
    "\n",
    "    # Second Conv Layer\n",
    "    model2.add(Conv2D(\n",
    "        filters = 128,\n",
    "        kernel_size = (3,3),\n",
    "        activation = \"relu\",\n",
    "        padding = \"same\"\n",
    "        \n",
    "    ))\n",
    "    model2.add(MaxPooling2D(pool_size = 2)) # 2 x 2 pooling shape of the max pooling\n",
    "\n",
    "\n",
    "    # Third Conv Layer\n",
    "    model2.add(Conv2D(\n",
    "        filters = 256,\n",
    "        kernel_size = (3,3),\n",
    "        activation =\"relu\",\n",
    "        padding = \"same\"\n",
    "    \n",
    "    ))\n",
    "    model2.add(MaxPooling2D(pool_size = 2)) # 2 x 2 pooling shape of the max pooling\n",
    "    \n",
    "    model2.add(Flatten())\n",
    "    \n",
    "    # Tuning Dnese Layer\n",
    "    model2.add(Dense(\n",
    "        units = hp.Int(\"dense_units\", 64, 256, step = 64), # Tuning no. of neurons\n",
    "        activation = \"relu\"\n",
    "        \n",
    "    ))\n",
    "    \n",
    "\n",
    "    # Tuning Dropout\n",
    "    model2.add(Dropout(\n",
    "        hp.Float(\"dropout\", 0.2, 0.5, step = 0.1)\n",
    "        \n",
    "    ))\n",
    "\n",
    "    \n",
    "    # Defining Output Layer\n",
    "    model2.add(Dense(num_classes, activation = \"softmax\"))\n",
    "\n",
    "    # Compiling model\n",
    "    model2.compile(\n",
    "        optimizer = keras.optimizers.Adam(\n",
    "            hp.Float(\"lr\", 1e-5, 1e-2, sampling = \"log\")),\n",
    "            loss = \"sparse_categorical_crossentropy\",\n",
    "            metrics = [\"accuracy\"]\n",
    "    )\n",
    "\n",
    "    return model2\n",
    "\n",
    "\n",
    "# Tuner\n",
    "\n",
    "tuner = kt.BayesianOptimization(\n",
    "    model_builder,\n",
    "    objective = \"val_accuracy\",\n",
    "    max_trials = 26,\n",
    "    directory = \"bo_tuner\",\n",
    "    project_name = \"cnn_fixed_layers\"\n",
    "    \n",
    ")\n",
    "\n",
    "# EarlyStopping\n",
    "es = EarlyStopping(\n",
    "    monitor = \"val_loss\",\n",
    "    patience = 5,\n",
    "    restore_best_weights = True\n",
    ")\n",
    "\n",
    "# Compute class weights\n",
    "class_weights_array = compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=np.unique(train_gen_small.classes),# giving this to capture distinct classes in the data.\n",
    "    y=train_gen_small.classes # Now, '.classes' will compute the class weights for the distinct classes instead of computing weights for all the records of train_gen.\n",
    ")\n",
    "\n",
    "\n",
    "class_weights_dict = dict(enumerate(class_weights_array))\n",
    "\n",
    "\n",
    "# Runinig Search\n",
    "tuner.search(train_gen_small,\n",
    "            validation_data = validation_gen_small,\n",
    "            epochs = 20,\n",
    "            class_weight = class_weights_dict,\n",
    "            callbacks = [es]\n",
    "            \n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad4e94ab-bcf0-46c8-9a7a-328425ec3163",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "sample_weights = list(class_weights_dict.values)\n",
    "sample_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "006c896c",
   "metadata": {},
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bfc11eb-9ccf-4811-9d41-bc60feeac5eb",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(keras.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "757f80f0",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "loss, acc = model.evaluate(validation_gen)\n",
    "print(\"Validation Accuracy: \", acc)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 8376592,
     "sourceId": 13215821,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8376801,
     "sourceId": 13216120,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31090,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
